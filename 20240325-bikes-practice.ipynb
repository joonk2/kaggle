{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3948,"databundleVersionId":32624,"sourceType":"competition"},{"sourceId":7931145,"sourceType":"datasetVersion","datasetId":4661837}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# referred all from \n[Bike Sharing Demand - Detailed EDA - Periodic Feat](https://www.kaggle.com/code/mexwell/bike-sharing-demand-detailed-eda-periodic-feat)","metadata":{}},{"cell_type":"markdown","source":"# table of contents\n## 1. data processing\n## 2. EDA\n### 2-1) distribution - target\n### 2-2) distribution - atemp & temp\n### 2-3) distribution - datetime\n### 2-4 sin cos graph\n## 3. rmse","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom prophet import Prophet\nfrom datetime import datetime \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:50:50.329403Z","iopub.execute_input":"2024-03-24T15:50:50.329848Z","iopub.status.idle":"2024-03-24T15:50:50.338201Z","shell.execute_reply.started":"2024-03-24T15:50:50.329817Z","shell.execute_reply":"2024-03-24T15:50:50.336591Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsrc = '/kaggle/input/bike-sharing-demand'\ntrain = pd.read_csv(src + '/train.csv') \ntest = pd.read_csv(src + '/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:50:50.340363Z","iopub.execute_input":"2024-03-24T15:50:50.340755Z","iopub.status.idle":"2024-03-24T15:50:50.391416Z","shell.execute_reply.started":"2024-03-24T15:50:50.340724Z","shell.execute_reply":"2024-03-24T15:50:50.389720Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stdout","text":"/kaggle/input/bike-sharing-demand/sampleSubmission.csv\n/kaggle/input/bike-sharing-demand/train.csv\n/kaggle/input/bike-sharing-demand/test.csv\n/kaggle/input/bikes-practice-jk/20240224titanic.ipynb\n/kaggle/input/bikes-practice-jk/20240303apple-quality-tutorial.ipynb\n/kaggle/input/bikes-practice-jk/e_car.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"train.T","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:50:50.393120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"checking on **std**, definately 4columns down below are good to train <br>\nsince they don't have a big gap\n`season`, `holiday`, `working day`, `weather` <br>\nstd >= 1 `-->` pretty much scattered ","metadata":{}},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape # (10886, 12)\ntest.shape # (6493, 9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all = pd.concat([train, test], ignore_index=True, axis = 0)\n# all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['season'].unique() # array([1, 2, 3, 4])\ntrain['holiday'].unique() # array([0, 1])\ntrain['workingday'].unique() # array([0, 1])\ntrain['weather'].unique() # array([1, 2, 3, 4])\ntrain['temp'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['humidity'].value_counts().head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"about bike demands, temp in top 10 is above <br>\ni find there's no connection","metadata":{}},{"cell_type":"code","source":"train['atemp'].value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"wind chill in top 10 <br>\n( &nbsp;&nbsp; // &nbsp;&nbsp; )","metadata":{}},{"cell_type":"code","source":"cat = ['season', 'holiday', 'workingday', 'weather']\nfor c in cat:\n    train[c] = train[c].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"season_dict = {1:'Spring', 2:'Summer', 3:'Fall', 4:'Winter'}\nweather_dict = {1:'Clear', 2:'Misty+Cloudy', 3:'Light Snow/Rain', 4:'Heavy Snow/Rain'}\ntrain['season'] = train['season'].map(season_dict)\ntrain['weather'] = train['weather'].map(weather_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"⬇️let's see if it's got an update","metadata":{}},{"cell_type":"code","source":"train['season'].unique()\n# Categories (4, object): ['Spring', 'Summer', 'Fall', 'Winter']\n\ntrain['weather'].unique()\n# Categories (4, object): ['Clear', 'Misty+Cloudy', 'Light Snow/Rain', 'Heavy Snow/Rain']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA","metadata":{}},{"cell_type":"markdown","source":"#### Distribution - Target <br>\nFirst of all check if we have any outliers in our `target`","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 5))\nsns.boxplot(train['count'])\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"like you see, full of outliers, lets delete'em by **IQR** ","metadata":{}},{"cell_type":"code","source":"def drop_outliers(df, target):\n    q1 = df[target].quantile(0.25)\n    q3 = df[target].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    df_filtered = df[(df[target] >= lower_bound) & (df[target] <= upper_bound)]\n    return df_filtered","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = drop_outliers(train, 'count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 5))\nsns.boxplot(train['count'])\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"strikingly noticeable! you see?","metadata":{}},{"cell_type":"code","source":"train.shape\n# 10886, 12 --> 10586, 12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"great, so we eliminated uneccessary parts","metadata":{}},{"cell_type":"code","source":"train['count'].value_counts().head(40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['count'].value_counts().head(40).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nsns.distplot(train['count'])\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target is highly skewed to the right. <br>\nWe have to think about transforming. <br>\nLater I will use logsqr since our evaluation metric is NLMSE.","metadata":{}},{"cell_type":"markdown","source":"# Distribution - Target vs. Category variables","metadata":{}},{"cell_type":"code","source":"group_season = pd.DataFrame(train.groupby(['season'])['count'].mean()).reset_index()\ngroup_holiday = pd.DataFrame(train.groupby(['holiday'])['count'].mean()).reset_index()\ngroup_workingday = pd.DataFrame(train.groupby(['workingday'])['count'].mean()).reset_index()\ngroup_weather = pd.DataFrame(train.groupby(['weather'])['count'].mean()).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 12))\naxes = fig.add_subplot(2, 2, 1)\nsns.barplot(data=group_season, x='season', y='count', ax=axes, palette='Set1')\naxes.set(xlabel='Season', ylabel='Count', title='Average count across seasons')\n\naxes = fig.add_subplot(2, 2, 2)\nsns.barplot(data=group_holiday, x='holiday', y='count', ax=axes, palette='Set1')\naxes.set(xlabel='Holiday', ylabel='Count', title='Average count across holidays')\n\naxes = fig.add_subplot(2, 2, 3)\nsns.barplot(data=group_workingday, x='workingday', y='count', ax=axes, palette='Set1')\naxes.set(xlabel='Workingday', ylabel='Count', title='Average count across workingday')\n\naxes = fig.add_subplot(2, 2, 4)\nsns.barplot(data=group_weather, x='weather', y='count', ax=axes, palette='Set1')\naxes.set(xlabel='Weather', ylabel='Count', title='Average count across weather conditions')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_data = train[['count', 'temp', 'atemp', 'humidity', 'windspeed']]\nsns.pairplot(weather_data, palette='Set1')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"there're 2 varaibles --> temp, atemp showing perfect linear pattern,<br>\nwhich means those two parameters are same, so we'd better remove one of'em <br>\nsince it's giving a tip that there's multicollinearity(다중공선성). <br>\ntherefore let's draw regression plots(회귀도표) to see linear relationship between those two variables <br><br>\n\n\n두 변수인 온도(temp)와 체감온도(atemp) 사이에 거의 완벽한 선형 패턴이 있으며, 이는 이러한 변수들 간의 다중공선성(multicollinearity)의 존재를 시사한다는 것을 나타냅니다. 다중공선성은 회귀 분석에서 독립 변수들 간에 강한 상관 관계가 있는 경우에 발생하는 문제로, 이 경우 한 독립 변수가 다른 독립 변수와 상당히 유사하거나 의존적인 경우 발생 <br>\n\n따라서, 두 변수 중 하나를 제거해야 할 필요가 있습니다. 회귀 도표(regression plots)를 빠르게 그려보는 것이 유용하며, 이 도표는 이름에서 알 수 있듯이 두 변수 사이의 회귀선을 생성하고 그들의 선형 관계를 시각화하는 데 도움이 됩니다.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(25, 12))\naxes = fig.add_subplot(1, 4, 1)\nsns.regplot(data=weather_data, x='temp', y='count', ax=axes, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\naxes = fig.add_subplot(1, 4, 2)\nsns.regplot(data=weather_data, x='atemp', y='count', ax=axes, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\naxes = fig.add_subplot(1, 4, 3)\nsns.regplot(data=weather_data, x='humidity', y='count', ax=axes, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\naxes = fig.add_subplot(1, 4, 4)\nsns.regplot(data=weather_data, x='windspeed', y='count', ax=axes, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n1. 온도 변수 중 하나를 제거할 수 있으며, 여기서는 atemp를 선택했습니다.\n2. 회귀 도표(regplots)는 자전거 대여량(count)과 온도(temp) 및 풍속(windspeed) 간에 양의 상관 관계가 있음을 나타냅니다.\n3. 회귀 도표는 자전거 대여량(count)과 습도(humidity) 간에 음의 상관 관계가 있음을 나타냅니다.\n\n1,2,3 요약 --> target인 자전거 대여량(count)은 매우 왜곡되어 있으므로 변형해야 합니다.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n### Datetime Features\nThe column datetime holds a lot of information in it. Let´s extract some datetime features from that column.","metadata":{}},{"cell_type":"code","source":"train['datetime'] = pd.to_datetime(train.datetime)\ntrain['hour'] = train.datetime.dt.hour\ntrain['month'] = train.datetime.dt.month\ntrain['year'] = train.datetime.dt.year\ntrain['date'] = train.datetime.dt.date\ntrain['weekday'] = train.datetime.dt.weekday","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axes = plt.subplots(1, 1, figsize=(20, 10))\nsns.boxplot(data=train, y='count', x='hour', hue='workingday', ax=axes, palette='Set1')\nhandles, _ = axes.get_legend_handles_labels()\naxes.legend(handles, ['Not a Working Day', 'Working Day'])\naxes.set(title='Hourly Count based on Working day or not')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see a clear pattern. <br>\nAt night, the demand for bicycles is relatively low. <br>\nThe slightly higher values when there is no working day are probably due to the fact that people are having a picnic. <br>\nOn a working day, demand is highest during the typical rush hour. <br>\nOn the other hand, when it is not a working day, the demand is higher from noon on.","metadata":{}},{"cell_type":"markdown","source":"## Monthly Distribution\n","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(nrows=1, ncols=1, figsize=(30, 10))\nagg_month = pd.DataFrame(train.groupby(['month', 'workingday'])['count'].mean().reset_index())\nsns.barplot(data=agg_month, x='month', y='count', hue='workingday', ax=axes, palette='Set1')\naxes.set(xlabel='Month', ylabel='Count', title='Average bike rentals per Month')\nhandles, _ = axes.get_legend_handles_labels()\naxes.legend(handles, ['Not a Working Day', 'Working Day'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"most active rental counts during months consdiered warm weather","metadata":{}},{"cell_type":"markdown","source":"## Quarterly Distribution","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(nrows=1, ncols=1, figsize=(30, 10))\nagg_quarter = pd.DataFrame(train.groupby(['season', 'workingday'])['count'].mean().reset_index())\nsns.barplot(data=agg_quarter, x='season', y='count', hue='workingday', ax=axes, palette='Set1')\naxes.set(xlabel='season', ylabel='Count', title='Average bike rentals per Quarter')\nhandles, _ = axes.get_legend_handles_labels()\naxes.legend(handles, ['Not a Working Day', 'Working Day'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Higher reservations can be seen at around 8am and 5pm (office hours) and close to 0 reservations very early in the morning <br>\n\n- From the above plot we can see the 2 patterns across the hours in a day in bike rentals\n- Working Day: First pattern where there is a peak in the rentals at around 8am and another at around 5pm. These correspond to working local bikers who typically are registered and go to work on working day which are Monday to Friday\n- Non Working Day: Second pattern where there is more or less a uniform rentals across the day with a peak at around noon time. These correspond to probably tourists who typically are casual users who rent/drop off bikes uniformly during the day and tour the city of Washington on non working days which typically are Saturday and Sunday\n- Also, we can see that we have more bike rentals during the Fall (July to September) and Summer (April to June) Season.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering \nPeriodic Features\nPeriodic features are those that repeat their values at regular intervals, like the hour, the days of a week, and the months of a year.\n\nWith cyclical or periodic features, values that are very different in absolute magnitude are actually close. For example, January is close to December, even though their absolute magnitude suggests otherwise.\n\nWe can use periodic functions like sine and cosine, to transform cyclical features and help machine learning models pick up their intrinsic nature.\n\nWe will apply periodic feature transformation to following features:\n\n- Hour\n- Month\n- Weekday","metadata":{}},{"cell_type":"code","source":"def periodic_transform(df, variable):\n    df[f\"{variable}_sin\"] = np.sin(df[variable] / df[variable].max() * 2 * np.pi)\n    df[f\"{variable}_cos\"] = np.cos(df[variable] / df[variable].max() * 2 * np.pi)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = periodic_transform(train, 'hour')\ntrain = periodic_transform(train, 'month')\ntrain = periodic_transform(train, 'weekday')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 15))\naxes = fig.add_subplot(3, 2, 1)\nsns.scatterplot(data=train, x='hour', y='hour_sin', ax=axes, color='red')\naxes = fig.add_subplot(3, 2, 2)\nsns.scatterplot(data=train, x='hour', y='hour_cos', ax=axes, color='red')\naxes = fig.add_subplot(3, 2, 3)\nsns.scatterplot(data=train, x='month', y='month_sin', ax=axes, color='blue')\naxes = fig.add_subplot(3, 2, 4)\nsns.scatterplot(data=train, x='month', y='month_cos', ax=axes, color='blue')\naxes = fig.add_subplot(3, 2, 5)\nsns.scatterplot(data=train, x='weekday', y='weekday_sin', ax=axes, color='green')\naxes = fig.add_subplot(3, 2, 6)\nsns.scatterplot(data=train, x='weekday', y='weekday_cos', ax=axes, color='green')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"사인함수, 코사인함수가\n변화가 없다가 갑자기 곡선을 그리며 급격하게 바뀐다는 것: <br>\n영향을 크게 받았다는 뜻 <br>\n쉽게 말해 천천히 내려가다 급격하게 증가 or <br>\n천천히 올라가다 급격하게 감소  <br>\nex) 시간 or 계절에 따른 대여량 증가","metadata":{}},{"cell_type":"markdown","source":"# Cleanup\nDrop columns <br>\nLet´s carefully check the dataframe again and identify columns which we can drop.","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns=['season',\n                            'atemp',\n                            'windspeed',\n                            'casual',\n                            'registered',\n                            'datetime',\n                            'date',\n                            'hour',\n                            'month',\n                            'weekday'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transform target","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nsns.distplot(train['count'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we will use `np.log` to transform our target.","metadata":{}},{"cell_type":"markdown","source":"### np.log \n데이터의 분포를 정규분포에 더 가깝게 만들 수 있습니다. <br>\n이는 모델이 데이터를 더 잘 이해하고 예측할 수 있도록 돕습니다. <br>\n로그 변환은 특히 분산이 지나치게 크거나 오른쪽 꼬리가 긴 경우에 유용합니다. <br>\n로그 변환을 통해 데이터를 변환하면 데이터의 스케일이 조정되고 이상치의 영향이 줄어들 수 있습니다.","metadata":{}},{"cell_type":"code","source":"train['count'] = np.log(train['count'])\nfig = plt.figure(figsize=(10, 10))\nsns.distplot(train['count'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns=['count'])\ny = train['count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(y_true, y_pred):\n        L1 = np.log1p(np.exp(y_true))\n        L2 = np.log1p(np.exp(y_pred))    \n        calc = (L1 - L2) ** 2\n        return np.sqrt(np.mean(calc))\n\ndef print_score(y, pred):\n    rmsle_val = rmsle(y, pred)\n    print(f'RMSLE: {rmsle_val}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1702)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'task': 'train', \n    'boosting': 'gbdt',\n    'objective': 'regression',\n    'num_leaves': 15,\n    'max_depth': 3,\n    'learning_rate': 0.1,\n    'metric': {'l2','l1'},\n    'verbose': -1\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade lightgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lgb.train(params,\n                 num_boost_round=300,\n                 train_set=lgb_train,\n                 valid_sets=lgb_eval,\n#                  early_stopping=20\n                 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val)\ny_pred = np.exp(y_pred)\ny_val_exp = np.exp(y_val)\n\nprint(f'MAE: {mean_absolute_error(y_val_exp, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_val_exp, y_pred)}')\nprint(f'RMSE: {mean_squared_error(y_val_exp, y_pred, squared=False)}')\nprint_score(y_val_exp, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 10))\nsns.scatterplot(x=y_val_exp, y=y_pred)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"this linear pattern means that predict values are almost simmilar with actual value","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning using RandomSearchCV","metadata":{}},{"cell_type":"code","source":"param_grid = {\n    'boosting_type': ['gbdt', 'dart'],\n    'num_leaves': [10, 20, 30],\n    'learning_rate': [0.01, 0.1, 0.5],\n    'n_estimators': [100, 200, 300],\n    'subsample': [0.5, 0.8, 1.0],\n    'colsample_bytree': [0.5, 0.8, 1.0],\n    'reg_alpha': [0.0, 0.1, 0.5],\n    'reg_lambda': [0.0, 0.1, 0.5]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor = lgb.LGBMRegressor()\nrandom_search = RandomizedSearchCV(estimator=regressor, param_distributions=param_grid,\n                                   n_iter=10, scoring='neg_mean_squared_error', cv=5)\nrandom_search.fit(X, y)\nprint(\"Best Parameters: \", random_search.best_params_)\nprint(\"Best MSE: \", -random_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = random_search.best_estimator_\ny_pred = best_model.predict(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.exp(y_pred)\ny_val_exp = np.exp(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'MAE: {mean_absolute_error(y_val_exp, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_val_exp, y_pred)}')\nprint(f'RMSE: {mean_squared_error(y_val_exp, y_pred, squared=False)}')\nprint_score(y_val_exp, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot feature importance\nfeature_imp = pd.DataFrame(sorted(zip(best_model.feature_importances_,X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20, 10))\nsns.scatterplot(x=y_val_exp, y=y_pred)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on testset","metadata":{}},{"cell_type":"markdown","source":"Transform testset <br>\nWe have to apply all transformation we did on the trainset to the testset.","metadata":{}},{"cell_type":"code","source":"cats = ['season', 'holiday', 'workingday', 'weather']\nfor cat in cats:\n    test[cat] = test[cat].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['datetime'] = pd.to_datetime(test.datetime)\ntest['hour'] = test.datetime.dt.hour\ntest['month'] = test.datetime.dt.month\ntest['year'] = test.datetime.dt.year\ntest['weekday'] = test.datetime.dt.weekday","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_datetime = test['datetime']\ntest = test.drop(columns=['datetime', \n                          'season', \n                          'atemp', \n                          'windspeed', \n                          'datetime', \n                          'hour', \n                          'month', \n                          'weekday'])\ntest.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'X_train shape: {X_train.shape}')\nprint(f'Test shape: {test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run prediction","metadata":{}},{"cell_type":"code","source":"test_preds = best_model.predict(test)\ntest_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### submit","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'datetime': sub_datetime, \"count\": [max(0, x) for x in np.exp(test_preds)]})\nsubmission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('bike-sharing-demand-prediction.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}